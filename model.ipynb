{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucsd-cse-spis-2023/ucsd-cse-spis-2023-peter-ritvik-project/blob/main/Categorical_output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown 1xE_aw7Ys5TGFuFBSRc2PwSVX1QuHQKP5     #download the csv file from drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hgO3t5Ds4Jx",
        "outputId": "ac00c56d-a3c7-46d7-a2c1-450d2dc05d2e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xE_aw7Ys5TGFuFBSRc2PwSVX1QuHQKP5\n",
            "To: /content/tripadvisor_hotel_reviews.csv\n",
            "\r  0% 0.00/15.0M [00:00<?, ?B/s]\r100% 15.0M/15.0M [00:00<00:00, 165MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas    #install pandas librry"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDgs97ctaChW",
        "outputId": "5aba8e87-ccda-4941-d608-136f398e8917"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np    #importing numpy and pandas\n",
        "import pandas as pd\n",
        "from tensorflow import keras    #importing keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer     #importing tokenizer to tokenize the words in the reviews data set\n",
        "from tensorflow.keras.models import Sequential  #builds a sequential neural network layer by layer\n",
        "from tensorflow.keras.layers import Dense #forms a layer of the neural network\n",
        "from tensorflow.keras.layers import Softmax  #converts the outputted vecot values to probabilities\n",
        "from keras.callbacks import EarlyStopping    #stops the model early if it detects overfitting\n",
        "from tensorflow.keras.layers import Dropout #drops a certain ammount of neurons from the layer\n",
        "from tensorflow.keras.regularizers import l2   #penalyzes large weights in the trainign data\n",
        "\n",
        "data = pd.read_csv('tripadvisor_hotel_reviews.csv') #reads the csv file\n",
        "\n",
        "reviews = data['Review'].tolist()      # adding reviews and ratings to list\n",
        "ratings = data['Rating'].tolist()\n",
        "\n",
        "\n",
        "training_reviews = data['Review'][:16000].tolist() #first 160000 reviews are training data\n",
        "testing_reviews = data['Review'][16000:].tolist()  #last 4000 reviews are testing data\n",
        "\n",
        "# Subtract 1 so each review's index is the review value - 1\n",
        "training_ratings = data['Rating'][:16000].to_numpy() - 1  #first 160000 ratings are training data\n",
        "testing_ratings = data['Rating'][16000:].to_numpy() - 1   #last 4000 are testing data\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(reviews)   #tokenizer analyzes the list 'reviews'\n",
        "vocab_size = len(tokenizer.word_index) + 1    #calculates the total number of unique words in the data set\n",
        "\n",
        "\n",
        "encoded_training_reviews = tokenizer.texts_to_matrix(training_reviews, mode='binary') #the neural netwrok needs numerical data so we encode the reviews to a binary matrix\n",
        "encoded_testing_reviews = tokenizer.texts_to_matrix(testing_reviews, mode='binary')\n",
        "\n",
        "\n",
        "\n",
        "categorical_training_ratings = keras.utils.to_categorical(training_ratings) #makes the training and testing ratings catorigical data\n",
        "categorical_testing_ratings = keras.utils.to_categorical(testing_ratings)"
      ],
      "metadata": {
        "id": "LQSMIM5qdi7v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(1024, activation='relu', input_shape=(vocab_size,), kernel_regularizer=l2(0.03))) #first layer of neural netwrok, 1024 neurons, all tokens inputed, regularized by multiplyer\n",
        "model.add(Dense(256, activation='relu', input_shape=(1024,)))   #hidden layer of neural network with 256 neurons\n",
        "model.add(Dropout(0.3))                                         # drops 30% of the neurons so thta genralized connections can be made (prevents overfitting)\n",
        "model.add(Dense(128, activation='relu', input_shape=(256,)))    #hiden layer of neural netowrk with 128 neurons\n",
        "model.add(Dense(64, activation='relu', input_shape=(128,)))     # hidden layer of neural network with 64 neurons\n",
        "model.add(Dense(5, activation='softmax'))  #outputs probabilities into 5 classes\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])    #optimized data, calculates catagorical loss in accuracy, measures accuracy\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)   #stops the model from going through all epochs after minimal difference in the validation loss is detected 3 times\n",
        "model.fit(encoded_training_reviews, categorical_training_ratings, validation_split=0.2,callbacks=[early_stopping], epochs=50, batch_size=64)    #tests 20% of the predicted data against testing data each time to see if overfitting is happening"
      ],
      "metadata": {
        "id": "lNsArs4ilP5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(encoded_testing_reviews, categorical_testing_ratings) #checks the loss and validation accuracy"
      ],
      "metadata": {
        "id": "EGMOFD5tJa2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(encoded_testing_reviews).argmax(axis=1) #find the index with the highest probability prediction along the axis of predicitons\n",
        "y_test = categorical_testing_ratings.argmax(axis=1) #finds the testing data in the class with the highest probability"
      ],
      "metadata": {
        "id": "05KP9IOZi1dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn import metrics #metrics from sklearn to set up confiusion matrix\n",
        "\n",
        "matrix = metrics.confusion_matrix(y_test, y_pred, normalize='all')\n",
        "matrix_display = metrics.ConfusionMatrixDisplay(matrix)\n",
        "matrix_display.plot() #displays confusion matrix showing how far predictions strayed aaway from actual ratings"
      ],
      "metadata": {
        "id": "aJCOK2LJiprM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "good_reviews = [input(\"Enter a positive hotel review --> \")]  #user inputs both a good and bad review\n",
        "bad_reviews = [input(\"Enter a negative hotel review --> \")]\n",
        "\n",
        "encoded_good_reviews = tokenizer.texts_to_matrix(good_reviews, mode='binary')\n",
        "predicted_good_ratings = model.predict(encoded_good_reviews)    #user inputed good review is encoded as binary matrix\n",
        "\n",
        "encoded_bad_reviews = tokenizer.texts_to_matrix(bad_reviews, mode='binary')   #user inputed bad review is encoded as binary matrix\n",
        "predicted_bad_ratings = model.predict(encoded_bad_reviews)\n",
        "\n",
        "\n",
        "testing_encoded_ratings = np.array(testing_ratings) / 5.0   #testing ratings are converted from 1-5 to 0-1\n",
        "testing_encoded_reviews = tokenizer.texts_to_matrix(testing_reviews, mode='binary')  #testing reviews are converted to binary\n",
        "\n",
        "predicted_good_ratings_list = predicted_good_ratings.flatten().tolist() #matrix is flatened and made into a list\n",
        "predicted_good_rating = 0 #place holder initial values of 0 are ste before the code enters the for loop\n",
        "max_good_probability = 0\n",
        "for i in range(len(predicted_good_ratings_list)):   #for values in the range from 0 to the end of the list\n",
        "    if predicted_good_ratings_list[i] > max_good_probability:   #if any i value is greater than the current max probability\n",
        "      predicted_good_rating = i+1   #the i value is increased by 1 and saved as the new max probability\n",
        "      max_good_probability = predicted_good_ratings_list[i]\n",
        "print(\"good review prediction: \", predicted_good_rating, \"stars\")\n",
        "\n",
        "predicted_bad_ratings_list = predicted_bad_ratings.flatten().tolist()\n",
        "predicted_bad_rating = 0\n",
        "max_bad_probability = 0\n",
        "for i in range(len(predicted_bad_ratings_list)):\n",
        "    if predicted_bad_ratings_list[i] > max_bad_probability:\n",
        "      predicted_bad_rating = i+1\n",
        "      max_bad_probability = predicted_bad_ratings_list[i]\n",
        "print(\"bad review prediction: \",predicted_bad_rating , \"stars\")\n"
      ],
      "metadata": {
        "id": "z0M-qFnxcr2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XuQKCo3lZCXy"
      }
    }
  ]
}
